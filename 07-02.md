---
title: CNN Arcitectures
date: February 07, 2024
---

- First layer maybe identifies edge
- Then next layer will identiy more complexity
- Receptive field increases, abstraction increases, and more complex features are identified as we go down the layers

**Important** features of CNNs:

- Non-linearities
    - Without this we are just building a linear model
    - Images are highly non-linear. A linear combination of two images would have both objects equally (but occlusion and all takes place). So we have to approach with non-linearity.
    - Simple non-linearities like tanh, sigmoid and relu are often sufficient
    - Be careful while initializing model params. If using relu and you end up on zero side (-ve), zero gradients. Training is stuck then.
    - ReLU (Rectified linear unit)
        - very fast
        - gradients (good)
        - piece wise linear approximation

- Are convolutions shift invariant or shift equivariant?

- Max vs average pooling: is one better? when and why?  
- pooling by striding 

AlexNet, GoogLeNet.
